AWSTemplateFormatVersion: 2010-09-09
Description: >-
  This template creates Databricks workspace resources in your AWS account using the API account. The API account is required if you want to use either customer managed VPCs or customer managed keys for notebooks. For feature availability, contact your Databricks representative. (qs-1r0odiedc)
Metadata:
  QuickStartDocumentation:
    EntrypointName: "Parameters for deploying a workspace and creating a cross-account IAM role"
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Workspace configuration"
        Parameters:
          - AccountId
          - Username
          - Password
          - PricingTier
          - DeploymentName
          - AWSRegion
          - HIPAAparm
      - Label:
          default: "IAM role and S3 bucket configuration"
        Parameters:
          - TagValue
          - ExistingCrossAccountIAMRoleARN
          - ExistingCrossAccountIAMRoleName
          - NewCrossAccountIAMRoleName
          - BucketName
      - Label:
          default: "(Optional) Customer managed key configuration for notebooks (requires the enterprise tier)"
        Parameters:
          - NewKMSKeyAlias
          - KeyUseCases
          - KeyReuseForClusterVolumes
      - Label:
          default: "Quick Start configuration"
        Parameters:
          - QSS3BucketName
          - QSS3KeyPrefix
    ParameterLabels:
      AccountId:
        default: Databricks account ID
      Username:
        default: Workspace account email
      Password:
        default: Workspace account password
      PricingTier:
        default: Pricing tier of the workspace
      DeploymentName:
        default: Workspace deployment name
      AWSRegion:
        default: AWS Region of the Databricks workspace
      HIPAAparm:
        default: HIPAA tier account
      TagValue:
        default: IAM role tag
      IAMRoleARN:
        default: Cross-account IAM role ARN
      BucketName:
        default: Root S3 bucket name
      NewKMSKeyAlias:
        default: Alias for the customer managed AWS KMS key
      KeyUseCases:
        default: Use case for which to use the key
      KeyReuseForClusterVolumes:
        default: Encrypt cluster EBS volumes
      QSS3BucketName:
        default: Quick Start S3 bucket name
      QSS3KeyPrefix:
        default: Quick Start S3 key prefix

Outputs:
  CustomerManagedVPCIAMRoleARN:
    Description: ARN of the customer managed cross-account IAM role.
    Condition: CreateCrossAccountIAMRole
    Value: !Ref accessRoleCustomerManagedVPC
  S3BucketName:
    Description: Name of the S3 root bucket.
    Value: !Ref assetsS3Bucket
  CustomerManagedKeyId:
    Description: ID of the customer managed key object.
    Condition: CMKEncrypted
    Value: !GetAtt createCustomerManagedKey.CustomerManagedKeyId
  CredentialsId:
    Description: Credential ID.
    Value: !GetAtt createCredentials.CredentialsId
  ExternalId:
    Description: Databricks external ID.
    Value: !GetAtt createCredentials.ExternalId
  NetworkId:
    Description: Databricks network ID.
    Value: !GetAtt createNetworks.NetworkId
  StorageConfigId:
    Description: Storage configuration ID.
    Value: !GetAtt createStorageConfiguration.StorageConfigId
  WorkspaceURL:
    Description: URL of the workspace.
    Value: !Join
      - ''
      - - 'https://'
        - !GetAtt createWorkspace.DeploymentName
        - '.cloud.databricks.com'
  WorkspaceStatus:
    Description: Status of the requested workspace.
    Value: !GetAtt createWorkspace.WorkspaceStatus
  WorkspaceStatusMessage:
    Description: Detailed status description of the requested workspace.
    Value: !GetAtt createWorkspace.WorkspaceStatusMsg
  PricingTier:
    Description: Pricing tier of the workspace. For more information, see https://databricks.com/product/aws-pricing.
    Value: !GetAtt createWorkspace.PricingTier
  ClusterPolicyID:
    Description: Unique identifier for the cluster policy.
    Value: !GetAtt createWorkspace.ClusterPolicyId

Parameters:
  AccountId:
    Description: "Account must use the E2 version of the platform. For more information, see https://docs.databricks.com/getting-started/overview.html#e2-architecture." 
    AllowedPattern: '^[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}$'
    MinLength: '36'
    Type: String
    Default: aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee
  Username:
    Description: "Account email for authenticating the REST API. Note that this value is case sensitive."
    AllowedPattern: '^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+$'
    ConstraintDescription: Must be a valid email format.
    MinLength: '8'
    Type: String
  Password:
    Description: "Account password for authenticating the REST API. The minimum length is 8 alphanumeric characters."
    MinLength: '8'
    NoEcho: 'true'
    Type: String
  PricingTier:
    Description: "If you do not provide this, the API defaults to the highest pricing tier. For more information, see https://databricks.com/product/aws-pricing."
    AllowedValues:
       - STANDARD
       - PREMIUM 
       - ENTERPRISE
       - '' 
    Type: String
    Default: ''
  DeploymentName:
    Description: "Choose this value carefully. The deployment name defines part of the workspace subdomain (e.g., workspace-deployment-name.cloud.databricks.com). This value must be unique across all deployments in all AWS Regions. It cannot start or end with a hyphen. If your account has a deployment-name prefix, add the prefix followed by a hyphen. For more information, see https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-5-create-the-workspace."
    AllowedPattern: '^(([a-z0-9][a-z0-9-]*[a-z0-9])|([a-z0-9]))$'
    ConstraintDescription: You must enter a valid subdomain for the workspace.
    Type: String
  AWSRegion:
    Description: "AWS Region where the workspace is created. Note that customer managed keys to encrypt notebooks are not supported in the us-west-1 Region."
    MinLength: '9'
    AllowedValues:
       - ap-northeast-1
       - ap-south-1
       - ap-southeast-2
       - ca-central-1
       - eu-central-1
       - eu-west-1
       - eu-west-2
       - us-east-1
       - us-east-2
       - us-west-1
       - us-west-2
    Type: String
  HIPAAparm:
    Description: 'Entering "Yes" creates a template for creating clusters in the HIPAA account.'
    AllowedValues:
       - 'Yes'
       - 'No'
    Default: 'No'
    Type: String
  TagValue:
    Description: "All new AWS objects get a tag with the key name. Enter a value to identify all new AWS objects that this template creates. For more information, see https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html."
    MinLength: '1'
    Type: String
    Default: databricks-quickstart-cloud-formation
  ExistingCrossAccountIAMRoleARN: 
    Description: "Enter the ARN of an existing cross-account IAM role, ensuring it meets the requirements here: https://docs.databricks.com/administration-guide/account-api/iam-role.html"
    Type: String
    Default: ''
  ExistingCrossAccountIAMRoleName:
    Description: "Enter the role name of the existing cross-account IAM role"
    Type: String
    Default: ''
  NewCrossAccountIAMRoleName: 
    Description: "Enter a unique cross-account IAM role name. For more information, see https://docs.aws.amazon.com/IAM/latest/APIReference/API_CreateRole.html."
    Type: String
    Default: ''
  BucketName:
    Description: "Name of your S3 root bucket. Enter only alphanumeric characters. For more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html."
    AllowedPattern: '(?=^.{3,63}$)(?!xn--)([a-z0-9](?:[a-z0-9-]*)[a-z0-9])$'
    MinLength: '3'
    MaxLength: '63'
    Type: String
    ConstraintDescription: Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).
  VPCID:
    Description: "ID of your VPC in which to create the new workspace. Only enter a value if you use the customer managed VPC feature. The format is vpc-xxxxxxxxxxxxxxxx. If unspecified, Databricks creates a new workspace in a new VPC. For more information, see https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html."
    Type: String
    Default: ''
  SecurityGroupIDs:
    Description: "Name of one or more VPC security groups. Only enter a value if you set VPCID. The format is sg-xxxxxxxxxxxxxxxxx. Use commas to separate multiple IDs. Databricks must have access to at least one security group but no more than five. You can reuse existing security groups. For more information, see https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html."
    Type: String
    Default: '' 
  SubnetIDs:
    Description: "Enter at least two private subnet IDs [i.e. subnet-xxxx,subnet-yyyy]. Only enter a value if you set VPCID. Subnets cannot be shared with other workspaces or non-Databricks resources. Each subnet must be private, have outbound access, and a netmask between /17 and /25. The NAT gateway must have its own subnet that routes 0.0.0.0/0 traffic to an internet gateway. For more information, see https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html."
    Type: String
    Default: ''
  NewKMSKeyAlias:
    Description: "(Optional) Creates a new AWS KMS key with the given alias for encrypting data"
    Type: String
    Default: ''
  KeyUseCases:
    Description: "Configures customer managed encryption keys. Acceptable values are MANAGED_SERVICES, STORAGE, or BOTH. For more information, see https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-5-configure-customer-managed-keys-optional."
    Type: String
    Default: 'BOTH'
  KeyReuseForClusterVolumes:
    Description: 'Only enter a value if the use case is STORAGE or BOTH. Acceptable values are "True" and "False."'
    Type: String
  QSS3BucketName:
    Description: "S3 bucket for Quick Start assets. Use this if you want to customize the Quick Start. The bucket name can include numbers, lowercase letters, uppercase letters, and hyphens, but it cannot start or end with a hyphen (-)."
    AllowedPattern: '^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$'
    Default: aws-quickstart
    Type: String
    MinLength: '3'
    MaxLength: '63'
    ConstraintDescription: Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).
  QSS3KeyPrefix:
    Description: "S3 key prefix to simulate a directory for your deployment assets.  The prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slashes (/). For more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html."
    AllowedPattern: '^[0-9a-zA-Z-/]*$'
    Type: String
    Default: quickstart-databricks-unified-data-analytics-platform/
  ResourceOwner:
    Description: The value of the Owner tag in the created resources on AWS
    Type: String
    Default: Databricks_E2
  ResourcePrefix:
    Description: The prefix for the created resource names
    Type: String
    Default: DatabricksAccountAPI
  DBSPrivateLinkMode:
    Description: The Private Link setup mode for the Databricks workspace
    Type: String
    AllowedValues:
      - "Off"
      - "PublicAccessEnabled"
      - "PublicAccessDisabled"
    Default: "Off"
  
Mappings:

# VPC Endpoints to hit on the Databricks control plane side

  PrivateLink:
    us-east-1:
      "workspace": "com.amazonaws.vpce.us-east-1.vpce-svc-09143d1e626de2f04"
      "backend": "com.amazonaws.vpce.us-east-1.vpce-svc-00018a8c3ff62ffdf"
    us-east-2:
      "workspace": "com.amazonaws.vpce.us-east-2.vpce-svc-041dc2b4d7796b8d3"
      "backend": "com.amazonaws.vpce.us-east-2.vpce-svc-090a8fab0d73e39a6"
    us-west-1:
      "workspace": "UNSUPPORTED"
      "backend": "UNSUPPORTED"
    us-west-2:
      "workspace": "com.amazonaws.vpce.us-west-2.vpce-svc-0129f463fcfbc46c5"
      "backend": "com.amazonaws.vpce.us-west-2.vpce-svc-0158114c0c730c3bb"
    eu-west-1:
      "workspace": "com.amazonaws.vpce.eu-west-1.vpce-svc-0da6ebf1461278016"
      "backend": "com.amazonaws.vpce.eu-west-1.vpce-svc-09b4eb2bc775f4e8c"
    eu-west-2:
      "workspace": "com.amazonaws.vpce.eu-west-2.vpce-svc-01148c7cdc1d1326c"
      "backend": "com.amazonaws.vpce.eu-west-2.vpce-svc-05279412bf5353a45"
    eu-central-1:
      "workspace": "com.amazonaws.vpce.eu-central-1.vpce-svc-081f78503812597f7"
      "backend": "com.amazonaws.vpce.eu-central-1.vpce-svc-08e5dfca9572c85c4"
    ap-southeast-1:
      "workspace": "com.amazonaws.vpce.ap-southeast-1.vpce-svc-02535b257fc253ff4"
      "backend": "com.amazonaws.vpce.ap-southeast-1.vpce-svc-0557367c6fc1a0c5c"
    ap-southeast-2:
      "workspace": "com.amazonaws.vpce.ap-southeast-2.vpce-svc-0b87155ddd6954974"
      "backend": "com.amazonaws.vpce.ap-southeast-2.vpce-svc-0b4a72e8f825495f6"
    ap-northeast-1:
      "workspace": "com.amazonaws.vpce.ap-northeast-1.vpce-svc-02691fd610d24fd64"
      "backend": "com.amazonaws.vpce.ap-northeast-1.vpce-svc-02aa633bda3edbec0"
    ap-south-1:
      "workspace": "com.amazonaws.vpce.ap-south-1.vpce-svc-0dbfe5d9ee18d6411"
      "backend": "com.amazonaws.vpce.ap-south-1.vpce-svc-03fd4d9b61414f3de"
    ca-central-1:
      "workspace": "com.amazonaws.vpce.ca-central-1.vpce-svc-0205f197ec0e28d65"
      "backend": "com.amazonaws.vpce.ca-central-1.vpce-svc-0c4e25bdbcbfbb684"
Conditions:
# Set condition when VPC ID is provided by the user
  HasExistingVPC: !Not [!Equals [!Ref VPCID, '']]
# Set condition when VPC ID is NOT provided by the user
  CreateVPC: !Equals [!Ref VPCID, '']
# Set condition when cross-account role should be created by this template
  CreateCrossAccountIAMRole: !Not [!Equals [!Ref NewCrossAccountIAMRoleName, '']]
# Set condition when AWS KMS key ID is provided by the user. 
  CMKEncrypted: !Not [!Equals [!Ref NewKMSKeyAlias, '']]
# Test for STORAGE CMK use case
  IsClusterVolumeSet: !Equals [!Ref KeyReuseForClusterVolumes, 'True']
  EnablePrivateLink: !Not [ !Equals [!Ref DBSPrivateLinkMode, "Off"] ]
  PublicAccessForPrivateLink: !Equals [!Ref DBSPrivateLinkMode, "PublicAccessEnabled"]

Rules:
# 1. Validate the selected Region from drop-down matches the Region from the Console
  RunningTemplateFromDifferentRegionThanDropDown:
    Assertions:
    - Assert: !Equals [!Ref AWSRegion, !Ref 'AWS::Region']
      AssertDescription: Region from the AWS Management Console must match the selected Region from the drop-down menu.
# 2. Ensure that an IAM role name is given when no ARN is provided for an existing cross account IAM role.
  ExistingCrossAccountIAMRole:
    RuleCondition: !Not [!Equals [!Ref ExistingCrossAccountIAMRoleARN, '']]
    Assertions:
    - Assert: !Equals [!Ref NewCrossAccountIAMRoleName, '']
      AssertDescription: Name for cross-account IAM role must be given if an existing cross-account IAM Role ARN is not provided.
    - Assert: !Not [!Equals [!Ref ExistingCrossAccountIAMRoleName, '']]
      AssertDescription: IAM Role name for existing cross-account role must be specified.
# 3. Ensure that an existing cross-account IAM role ARN is not supplied when an IAM role name is given to create the cross-account role.
  CreateCrossAccountIAMRole:
    RuleCondition: !Not [!Equals [!Ref NewCrossAccountIAMRoleName, '']]
    Assertions:
    - Assert: !Equals [!Ref ExistingCrossAccountIAMRoleARN, '']
      AssertDescription: Existing IAM Role ARN should not be given when a new cross-account role name is provided.
# 4. Ensure that the security group ID is supplied when an existing VPC ID is provided.
  SecurityGroupIDExists:
    RuleCondition: !Not [!Equals [!Ref VPCID, '']]
    Assertions:
    - Assert: !Not [!Equals [!Ref SecurityGroupIDs, '']]
      AssertDescription: Security Group ID should be supplied when VPC ID is provided.
# 5. Ensure that subnet IDs are supplied when an existing VPC ID is provided.
  SubnetIDsExist:
    RuleCondition: !Not [!Equals [!Ref VPCID, '']]
    Assertions:
    - Assert: !Not [!Equals [!Ref SubnetIDs, '']]
      AssertDescription: Subnet IDs should be supplied when VPC ID is provided.
# 4. Optional Section. Ensure the KeyAlias is provided when a use case is specified.
  KeyUseCases1:
    RuleCondition: !Not [!Equals [!Ref NewKMSKeyAlias, '']]
    Assertions:
    - Assert: !Contains [['MANAGED_SERVICES', 'STORAGE', 'BOTH'],!Ref KeyUseCases]
      AssertDescription: Acceptable values are MANAGED_SERVICES, STORAGE, or BOTH when you provide a new key alias to be created.
    
  KeyUseCases2:
    RuleCondition: !Or
      - !Equals [!Ref KeyUseCases, 'STORAGE']
      - !Equals [!Ref KeyUseCases, 'BOTH']
    Assertions:
    - Assert: !Contains [['True', 'False'], !Ref KeyReuseForClusterVolumes]
      AssertDescription: 'Acceptable values are "True" and "False" when the use case is either STORAGE or BOTH.' 
 
  KeyUseCases3:
    RuleCondition: !Equals [!Ref KeyUseCases, 'MANAGED_SERVICES']
    Assertions: 
    - Assert: !Equals [!Ref KeyReuseForClusterVolumes, '']
      AssertDescription: Value must be null if MANAGED_SERVICES is specified.
        
Resources:
#
#  THE FOLLOWING RESOURCES ONLY APPLY IF THIS TEMPLATE IS CONFIGURING THE VPC
#
# Creates the VPC
  customerManagedVPC:
    Type: 'AWS::EC2::VPC'
    Condition: CreateVPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: 'true'
      EnableDnsSupport: 'true'

# Creates private subnet for deployment of EC2 compute, note that CidrBlock and AZ can be changed.
  privateSubnet1:
    Type: AWS::EC2::Subnet
    Condition: CreateVPC
    Properties:
      VpcId: !Ref customerManagedVPC
      AvailabilityZone: "us-east-1a"
      CidrBlock: 10.0.0.0/24

# Creates private subnet for deployment of EC2 compute, note that CidrBlock and AZ can be changed.
  privateSubnet2:
    Type: AWS::EC2::Subnet
    Condition: CreateVPC
    Properties:
      CidrBlock: 10.0.1.0/24
      VpcId: !Ref customerManagedVPC
      AvailabilityZone: "us-east-1b"

# Public subnet is required for outbound internet access for installation of external libraries (i.e. Maven, PyPI, CRAN)
  publicSubnet:
    Type: AWS::EC2::Subnet
    Condition: CreateVPC
    Properties:
      CidrBlock: 10.0.2.0/24
      VpcId: !Ref customerManagedVPC
      AvailabilityZone: "us-east-1c"
      MapPublicIpOnLaunch: 'true'

# Internet Gateway that is required to route outbound requests
  internetGateway:
    Type: AWS::EC2::InternetGateway
    Condition: CreateVPC

# Attaches Internet Gateway to VPC
  internetGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Condition: CreateVPC
    Properties:
      InternetGatewayId: !Ref internetGateway
      VpcId: !Ref customerManagedVPC

# Elastic IP address for the NAT Gateway
  NatGateway1EIP:
    Type: AWS::EC2::EIP
    Condition: CreateVPC
    DependsOn: internetGatewayAttachment
    Properties:
      Domain: vpc

# Specifies a network address translation (NAT) gateway in the specified subnet.
  NatGateway1:
    Type: AWS::EC2::NatGateway
    Condition: CreateVPC
    Properties:
      AllocationId: !GetAtt NatGateway1EIP.AllocationId
      SubnetId: !Ref publicSubnet

# RouteTable for public access
  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Condition: CreateVPC
    Properties:
      VpcId: !Ref customerManagedVPC

# Routtes all outbound public traffic to the internet gateway
  DefaultPublicRoute:
    Type: AWS::EC2::Route
    Condition: CreateVPC
    DependsOn: internetGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref internetGateway

# Associates Route table with public subnet for outbound public traffic
  PublicSubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Condition: CreateVPC
    Properties:
      RouteTableId: !Ref PublicRouteTable
      SubnetId: !Ref publicSubnet

# Private Route Table for private subnets
  PrivateRouteTable1:
    Type: AWS::EC2::RouteTable
    Condition: CreateVPC
    Properties:
      VpcId: !Ref customerManagedVPC

# Private Route, routes public traffic to the Nat Gateway
  DefaultPrivateRoute1:
    Type: AWS::EC2::Route
    Condition: CreateVPC
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NatGateway1

# Associate RouteTable with private subnet
  PrivateSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Condition: CreateVPC
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      SubnetId: !Ref privateSubnet1

# Associate RouteTable with private subnet
  PrivateSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Condition: CreateVPC
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      SubnetId: !Ref privateSubnet2

# Security Group
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Condition: CreateVPC
    Properties:
      VpcId: !Ref customerManagedVPC
      GroupDescription: Security group for Databricks E2 deployment

# Security Group Ingress, according to the following: https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html#security-groups
  SecurityGroupIngressTcp:
    Type: AWS::EC2::SecurityGroupIngress
    Condition: CreateVPC
    Properties:
      GroupId: !Ref SecurityGroup
      SourceSecurityGroupId: !Ref SecurityGroup
      IpProtocol: tcp
      FromPort: 0
      ToPort: 65535

# Security group Ingress, according to the following: https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html#security-groups
  SecurityGroupIngressUdp:
    Type: AWS::EC2::SecurityGroupIngress
    Condition: CreateVPC
    Properties:
      GroupId: !Ref SecurityGroup
      SourceSecurityGroupId: !Ref SecurityGroup
      IpProtocol: udp
      FromPort: 0
      ToPort: 65535

# Security Group Egress, according to the following: https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html#security-groups
  DBSWorkspaceSecurityGroupDefaultTcpEgress:
    Type: AWS::EC2::SecurityGroupEgress
    Condition: CreateVPC
    Properties:
      GroupId: !GetAtt SecurityGroup.GroupId
      Description: Allow all tcp output access from the same security group
      DestinationSecurityGroupId: !GetAtt SecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 0
      ToPort: 65535
  
# Allow all UDP Egress from the same security group
  DBSWorkspaceSecurityGroupDefaultUdpEgress:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      GroupId: !GetAtt SecurityGroup.GroupId
      Description: Allow all udp output access from the same security group
      DestinationSecurityGroupId: !GetAtt SecurityGroup.GroupId
      IpProtocol: udp
      FromPort: 0
      ToPort: 65535

# Allow access to external libraries, other cloud data sources from same security group  
  DBSWorkspaceSecurityGroupEgressForHttps:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      GroupId: !GetAtt SecurityGroup.GroupId
      Description: Allow accessing Databricks infrastructure, cloud data sources, and library repositories
      CidrIp: 0.0.0.0/0
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

# Allow access to the Databricks metastore
  DBSWorkspaceSecurityGroupEgressForMetastore:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      GroupId: !GetAtt SecurityGroup.GroupId
      Description: Allow accessing the Databricks metastore
      CidrIp: 0.0.0.0/0
      IpProtocol: tcp
      FromPort: 3306
      ToPort: 3306

# Allow access to the Databricks control plane using PrivateLink
  DBSWorkspaceSecurityGroupEgressForPrivateLink:
    Condition: EnablePrivateLink
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      GroupId: !GetAtt SecurityGroup.GroupId
      Description: Allow accessing the Databricks control plane using private link
      CidrIp: 0.0.0.0/0
      IpProtocol: tcp
      FromPort: 6666
      ToPort: 6666

# Network ACL
  NetworkAcl:
    Type: AWS::EC2::NetworkAcl
    Condition: CreateVPC
    Properties:
      VpcId: !Ref customerManagedVPC

# Network ACL Entry, adhering to the following: https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html#subnet-level-network-acls
  NetworkAclEntryInboundAllow:
    Type: AWS::EC2::NetworkAclEntry
    Condition: CreateVPC
    Properties:
      Egress: "false"
      CidrBlock: 0.0.0.0/0
      Protocol: -1
      RuleNumber: 100
      RuleAction: allow
      NetworkAclId: !Ref NetworkAcl

# Network ACL Entry, adhering to the following: https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html#subnet-level-network-acls
  NetworkAclEntryOutboundAllow:
    Type: AWS::EC2::NetworkAclEntry
    Condition: CreateVPC
    Properties:
      Egress: "true"
      CidrBlock: 0.0.0.0/0
      Protocol: -1
      RuleNumber: 100
      RuleAction: allow
      NetworkAclId: !Ref NetworkAcl

# Associate Subnet Network ACL with subnet
  SubnetNetworkAclNAT:
    Type: AWS::EC2::SubnetNetworkAclAssociation
    Condition: CreateVPC
    Properties:
      NetworkAclId: !Ref NetworkAcl
      SubnetId: !Ref publicSubnet

# Associate SubnetNetwork ACL with subnet
  SubnetNetworkAclPrivateSubnet1:
    Type: AWS::EC2::SubnetNetworkAclAssociation
    Condition: CreateVPC
    Properties:
      NetworkAclId: !Ref NetworkAcl
      SubnetId: !Ref privateSubnet1

  SubnetNetworkAclPrivateSubnet2:
    Type: AWS::EC2::SubnetNetworkAclAssociation
    Condition: CreateVPC
    Properties:
      NetworkAclId: !Ref NetworkAcl
      SubnetId: !Ref privateSubnet2

#
# THE FOLLOWING RESOURCES ARE USED FOR PRIVATELINK
#

# S3 Gateway endpoing for communication to asset bucket hosted in control plane.
  S3GatewayEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      ServiceName: !Sub com.amazonaws.${AWS::Region}.s3
      VpcEndpointType: Gateway
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      RouteTableIds:
        - !Ref PrivateRouteTable1
        - !Ref PublicRouteTable

# STS endpoint for private communication with STS hosted in control plane.
  STSInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      ServiceName: !Sub com.amazonaws.${AWS::Region}.sts
      VpcEndpointType: Interface
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      PrivateDnsEnabled: true
      SecurityGroupIds:
        - !GetAtt SecurityGroup.GroupId
      SubnetIds:
        - !Ref privateSubnet1
        - !Ref privateSubnet2

# Kinesis Interface endpoint for streaming log information
  KinesisInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      ServiceName: !Sub com.amazonaws.${AWS::Region}.kinesis-streams
      VpcEndpointType: Interface
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      PrivateDnsEnabled: true
      SecurityGroupIds:
        - !GetAtt SecurityGroup.GroupId
      SubnetIds:
        - !Ref privateSubnet1
        - !Ref privateSubnet2
  
  KinesisFirehoseInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      ServiceName: !Sub com.amazonaws.${AWS::Region}.kinesis-firehose
      VpcEndpointType: Interface
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      PrivateDnsEnabled: true
      SecurityGroupIds:
        - !GetAtt SecurityGroup.GroupId
      SubnetIds:
        - !Ref privateSubnet1
        - !Ref privateSubnet2

# KMS Interface endpoint
  KMSInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      ServiceName: !Sub com.amazonaws.${AWS::Region}.kms
      VpcEndpointType: Interface
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      PrivateDnsEnabled: true
      SecurityGroupIds:
        - !GetAtt SecurityGroup.GroupId
      SubnetIds:
        - !Ref privateSubnet1
        - !Ref privateSubnet2
   
#
# THE FOLLOWING RESOURCE ONLY APPLIES IF YOU WANT THIS TEMPLATE TO CREATE THE CROSS-ACCOUNT IAM ROLE.
#

# Create cross-account IAM role, according to the following: https://docs.databricks.com/administration-guide/account-api/iam-role.html

  accessRoleCustomerManagedVPC:
    Type: 'AWS::IAM::Role'
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - EIAMPolicyWildcardResource
          ignore_reasons:
            EIAMPolicyWildcardResource: "Must manage databricks workspaces."
    Condition: CreateCrossAccountIAMRole
    Properties:
      RoleName: !Ref NewCrossAccountIAMRoleName
      AssumeRolePolicyDocument:
        Statement:
          - Action: 'sts:AssumeRole'
            Condition:
               StringEquals: 
                'sts:ExternalId': !Sub '${AccountId}'
            Effect: Allow
            Principal:
              AWS: !Join 
                - ''
                - - 'arn:aws:iam::'
                  - '414351767826'
                  - ':root'
            Sid: ''
        Version: 2012-10-17
      Path: /
      Policies:
        - PolicyDocument:
            Statement:
              - Sid: Stmt1403287045000
                Effect: Allow
                Action:
                  - 'ec2:AssociateIamInstanceProfile'
                  - 'ec2:AttachVolume'
                  - 'ec2:AuthorizeSecurityGroupEgress'
                  - 'ec2:AuthorizeSecurityGroupIngress'
                  - 'ec2:CancelSpotInstanceRequests'
                  - 'ec2:CreateTags'
                  - 'ec2:CreateVolume'
                  - 'ec2:DeleteTags'
                  - 'ec2:DeleteVolume'
                  - 'ec2:DescribeAvailabilityZones'
                  - 'ec2:DescribeIamInstanceProfileAssociations'
                  - 'ec2:DescribeInstanceStatus'
                  - 'ec2:DescribeInstances'
                  - 'ec2:DescribeInternetGateways'
                  - 'ec2:DescribeNatGateways'
                  - 'ec2:DescribeNetworkAcls'
                  - 'ec2:DescribePrefixLists'
                  - 'ec2:DescribeReservedInstancesOfferings'
                  - 'ec2:DescribeRouteTables'
                  - 'ec2:DescribeSecurityGroups'
                  - 'ec2:DescribeSpotInstanceRequests'
                  - 'ec2:DescribeSpotPriceHistory'
                  - 'ec2:DescribeSubnets'
                  - 'ec2:DescribeVolumes'
                  - 'ec2:DescribeVpcAttribute'
                  - 'ec2:DescribeVpcs'
                  - 'ec2:DetachVolume'
                  - 'ec2:DisassociateIamInstanceProfile'
                  - 'ec2:ReplaceIamInstanceProfileAssociation'
                  - 'ec2:RequestSpotInstances'
                  - 'ec2:RevokeSecurityGroupEgress'
                  - 'ec2:RevokeSecurityGroupIngress'
                  - 'ec2:RunInstances'
                  - 'ec2:TerminateInstances'
                Resource:
                  - '*'
              - Effect: Allow
                Action:
                  - 'iam:CreateServiceLinkedRole'
                  - 'iam:PutRolePolicy'
                Resource: 
                  - !Sub arn:${AWS::Partition}:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot
                Condition:
                  StringLike:
                    'iam:AWSServiceName': spot.amazonaws.com
            Version: 2012-10-17
          PolicyName: databricks-cross-account-iam-role-policy     
      Tags:
        -
          Key: Name
          Value: !Sub '${TagValue}-IAMRole'

#
# THE FOLLOWING RESOURCES CONFIGURE THE S3 ROOT BUCKET AND THE ASSOCIATED POLICY.
#
# S3 root bucket requirements
  assetsS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls       : true
        BlockPublicPolicy     : true
        IgnorePublicAcls      : true
        RestrictPublicBuckets : true
      LifecycleConfiguration:
        Rules: 
          - Id: DeleteContentAfter5days
            Status: Enabled
            NoncurrentVersionExpirationInDays: 5
      VersioningConfiguration: 
        Status: Enabled
  
 # S3 root bucket policy 
  bucketPolicy:
    Type: 'AWS::S3::BucketPolicy'
    Properties: 
      PolicyDocument:
        Id: MyPolicy
        Version: 2012-10-17
        Statement:
          - Sid: Grant Databricks Access
            Effect: Allow
            Principal:
              AWS: arn:aws:iam::414351767826:root
            Action:
              - 's3:GetObject'
              - 's3:GetObjectVersion'
              - 's3:PutObject'
              - 's3:DeleteObject'
              - 's3:ListBucket'
              - 's3:GetBucketLocation'
            Resource:
              - !Sub 'arn:${AWS::Partition}:s3:::${assetsS3Bucket}/*'
              - !Sub 'arn:${AWS::Partition}:s3:::${assetsS3Bucket}'
      Bucket: !Ref assetsS3Bucket

#
# THE FOLLOWING RESOURCES DEPLOY A KMS KEY TO USE FOR ENCRYPTING CONTROL PLANE DATA AND/OR ENCRYPTING DATA PLANE STORAGE
#

# creation of KMS Key
  CustomerManagedKey:
    Type: 'AWS::KMS::Key'
    Properties:
      Description: KMS key used to encrypted Databricks data
      KeyPolicy:
        Version: 2012-10-17
        Id: key-default-1
        Statement:
          - Sid: Enable IAM user permissions
            Effect: Allow
            Principal: 
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'

# Creation of KMS Alias
  CustomerManagedKeyAlias:
    Type: 'AWS::KMS::Alias'
    Properties:
      AliasName: !Sub alias/${NewKMSKeyAlias}
      TargetKeyId: !GetAtt CustomerManagedKey.KeyId
#
# THE FOLLOWING RESOURCES INVOKE DATABRICKS APIs
#
  
# Databricks API for configuring notebook encryption with a customer managed AWS KMS, if provided, utilizes standard Databricks custom
# resources included in the Quick Start guide, 
# https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-5-configure-customer-managed-keys-optional
  createCustomerManagedKey:
    Condition: CMKEncrypted
    DependsOn: updateCustomManagedKeys
    Type: Custom::CreateCustomerManagedKey
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_CUSTOMER_MANAGED_KEY'
      accountId: !Ref AccountId
      key_arn: !GetAtt CustomerManagedKey.Arn
      key_alias: !Ref CustomerManagedKeyAlias
      use_cases: !Ref KeyUseCases
      reuse_key_for_cluster_volumes: !Ref KeyReuseForClusterVolumes
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      user_agent: 'databricks-CloudFormation-provider'

# Databricks API for workspace credentials, https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-credential-config
  createCredentials:
    Type: Custom::CreateCredentials
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_CREDENTIALS'
      accountId: !Ref AccountId
      credentials_name: !Join
        - '-'
        - - !Ref DeploymentName
          - 'credentials'
      role_arn: !If [CreateCrossAccountIAMRole, !GetAtt accessRoleCustomerManagedVPC.Arn, !Ref ExistingCrossAccountIAMRoleARN]
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      user_agent: 'databricks-CloudFormation-provider'

# Databricks API for workspace storage configuration, 
# https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-storage-config
  createStorageConfiguration:
    Type: Custom::CreateStorageConfigurations
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_STORAGE_CONFIGURATIONS'
      accountId: !Ref AccountId
      storage_config_name: !Join
        - '-'
        - - !Ref 'DeploymentName'
          - 'storage'
      s3bucket_name: !Ref assetsS3Bucket
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      user_agent: 'databricks-CloudFormation-provider'

# Databricks API for customer managed VPC
# https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-network-config
  createNetworks:
    DependsOn: createStorageConfiguration
    Type: Custom::createNetworks
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      DatabricksAccountId: !Ref AccountId
      Username: !Ref Username
      Password: !Ref Password
      NetworkName: !Sub ${AWS::StackName}_network
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      Subnets: !If [HasExistingVPC, !Ref SubnetIDs, !Join [',', [!Ref privateSubnet1, !Ref privateSubnet2]]]
      SecurityGroups: !If [HasExistingVPC, !Ref SecurityGroupIDs, !Ref SecurityGroup]
      VpcEndpoints: !If
        - EnablePrivateLink
        - RestApiEndpointId: !Ref WorkspaceVpcEndpoint
          DataplaneRelayEndpointId: !Ref BackendVpcEndpoint
        - !Ref AWS::NoValue

# Custom resource function to create a network configuration
# https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-network-config
  NetworkConfigurationFn:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: NetworkConfigurationFn
      Description: The custom resource function to create a network configuration
      Handler: index.lambda_handler
      MemorySize: 256
      Role: !GetAtt functionRole.Arn
      Runtime: python3.9
      Timeout: 25
      Code:
        ZipFile: |
          import cfnresponse
          import urllib3
          import json
          def checkForMissingProperty(properties, propertyName):
            if propertyName not in properties:
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No " + propertyName + " property specified")
              return True
            return False
          def lambda_handler(event, context):
            for propertName in ('DatabricksAccountId', 'Username', 'Password'):
              if checkForMissingProperty(event['ResourceProperties'], propertName): return
            accountsAPIBaseURL = 'https://accounts.cloud.databricks.com/api/2.0/accounts/' + event['ResourceProperties']['DatabricksAccountId']
            myHeaders = urllib3.util.make_headers(basic_auth = event['ResourceProperties']['Username'] + ':' + event['ResourceProperties']['Password']) | {'Content-Type': 'application/json'}
            http = urllib3.PoolManager()
            if event['RequestType'] == 'Create':
              if 'NetworkName' not in event['ResourceProperties']:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No NetworkName property specified")
                return
              if 'VpcId' not in event['ResourceProperties']:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No VpcId property specified")
                return
              if ('Subnets' not in event['ResourceProperties']) or type(event['ResourceProperties']['Subnets']) != list:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No Subnets property specified")
                return
              if ('SecurityGroups' not in event['ResourceProperties']) or type(event['ResourceProperties']['SecurityGroups']) != list:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No SecurityGroups property specified")
                return
              postData = {
                "network_name": event['ResourceProperties']['NetworkName'],
                "vpc_id": event['ResourceProperties']['VpcId'],
                "subnet_ids": event['ResourceProperties']['Subnets'].split(",")
                "security_group_ids": [event['ResourceProperties']['SecurityGroups']]
              }
              if ('VpcEndpoints' in event['ResourceProperties']) and (type(event['ResourceProperties']) == dict) and \
                ('RestApiEndpointId' in event['ResourceProperties']['VpcEndpoints']) and ('DataplaneRelayEndpointId' in event['ResourceProperties']['VpcEndpoints']) and \
                (event['ResourceProperties']['VpcEndpoints']['RestApiEndpointId'] is not None) and (len(event['ResourceProperties']['VpcEndpoints']['RestApiEndpointId']) > 0) and \
                (event['ResourceProperties']['VpcEndpoints']['DataplaneRelayEndpointId'] is not None) and (len(event['ResourceProperties']['VpcEndpoints']['DataplaneRelayEndpointId']) > 0):
                postData["vpc_endpoints"] = { "rest_api": [ event['ResourceProperties']['VpcEndpoints']['RestApiEndpointId'] ], "dataplane_relay": [ event['ResourceProperties']['VpcEndpoints']['DataplaneRelayEndpointId'] ] }
              response = http.request('POST', accountsAPIBaseURL + '/networks', headers=myHeaders, body = json.dumps(postData))
              if response.status != 201:
                errorMessage = 'Unknown Error'
                try: errorMessage = json.loads(response.data.decode())['message']
                except: errorMessage = response.reason
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = errorMessage)
                return
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, json.loads(response.data.decode())['network_id'])
              return
            if event['RequestType'] == 'Delete':
              response = http.request('DELETE', accountsAPIBaseURL + '/networks/' + event['PhysicalResourceId'], headers=myHeaders)
              if response.status != 200:
                errorMessage = 'Unknown Error'
                try: errorMessage = json.loads(response.data.decode())['message']
                except: errorMessage = response.reason
                print(errorMessage)
            cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

  # The log group on CloudWatch logs
  NetworkConfigurationLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${NetworkConfigurationFn}
      RetentionInDays: 365

# Workspace VPC Endpoint.
# Each Databricks control plane (typically one per region) publishes two AWS VPC endpoint services for PrivateLink.
# The workspace VPC endpoint service applies to both a Databricks front-end PrivateLink connection or the Databricks back-end PrivateLink connection for REST APIs.
  WorkspaceVpcEndpoint:
    Condition: EnablePrivateLink
    Type: Custom::WorkspaceVpcEndpoint
    Properties:
      ServiceToken: !GetAtt WorkspaceVpcEndpointFn.Arn
      DatabricksAccountId: !Ref AccountId
      Username: !Ref Username
      Password: !Ref Password
      EndpointName: !Sub ${AWS::StackName}_workspaceVpcEndpoint
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      Subnets: !If [HasExistingVPC, !Ref SubnetIDs, !Join [',', [!Ref privateSubnet1, !Ref privateSubnet2]]]
      SecurityGroups: !If [HasExistingVPC, !Ref SecurityGroupIDs, !Ref SecurityGroup]

# Custom resource definition for creating the workspace VPC endpoint
  WorkspaceVpcEndpointFn:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: WorkspaceVpcEndpointFn
      Description: The custom resource function to create a workspace VPC endpoint
      Handler: index.lambda_handler
      MemorySize: 256
      Role: !GetAtt functionRole.Arn
      Runtime: python3.9
      Timeout: 600
      Code:
        ZipFile: !Sub 
          - |
            import cfnresponse, urllib3, json, boto3, time
            region = "${AWS::Region}"
            serviceName = "${ServiceName}"
            ec2Client = boto3.session.Session(region_name=region).client('ec2')
            def waitForEndpointDeletion(vpcEndpointId):
              while True:
                try:
                  status = ec2Client.describe_vpc_endpoints(Filters=[{'Name':'vpc-endpoint-id', 'Values': [vpcEndpointId]}])['VpcEndpoints'][0]['State']
                  time.sleep(5)
                except Exception as e: break
            def waitForEndpoint(vpcEndpointId):
              while True: # Wait for the endpoint to become available
                try:
                  status = ec2Client.describe_vpc_endpoints(Filters=[{'Name':'vpc-endpoint-id', 'Values': [vpcEndpointId]}])['VpcEndpoints'][0]['State']
                  if status == 'available': break
                except Exception as e: print(str(e))
                time.sleep(5)
            def checkForMissingProperty(properties, propertyName):
              if propertyName not in properties:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No " + propertyName + " property specified")
                return True
              return False
            def lambda_handler(event, context):
              for propertName in ('DatabricksAccountId', 'Username', 'Password'):
                if checkForMissingProperty(event['ResourceProperties'], propertName): return
              accountsAPIBaseURL = 'https://accounts.cloud.databricks.com/api/2.0/accounts/' + event['ResourceProperties']['DatabricksAccountId']
              myHeaders = urllib3.util.make_headers(basic_auth = event['ResourceProperties']['Username'] + ':' + event['ResourceProperties']['Password']) | {'Content-Type': 'application/json'}
              http = urllib3.PoolManager()
              if event['RequestType'] == 'Create':
                for propertName in ('EndpointName', 'VpcId', 'Subnets', 'SecurityGroups'):
                  if checkForMissingProperty(event['ResourceProperties'], propertName): return
                vpcEndpointId = None
                try: # Create the endpoint in the VPC
                  vpcEndpointId = ec2Client.create_vpc_endpoint(VpcEndpointType='Interface', VpcId=event['ResourceProperties']['VpcId'],
                    ServiceName=serviceName, SubnetIds=event['ResourceProperties']['Subnets'].split(","), SecurityGroupIds=[event['ResourceProperties']['SecurityGroups']],
                    TagSpecifications=[{"ResourceType": "vpc-endpoint","Tags": [{"Key": "Owner", "Value": "${ResourceOwner}"}]}])['VpcEndpoint']['VpcEndpointId']
                except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = str(e))
                  return
                postData = {"vpc_endpoint_name": event['ResourceProperties']['EndpointName'], "aws_vpc_endpoint_id": vpcEndpointId, "region": region}
                response = http.request('POST', accountsAPIBaseURL + '/vpc-endpoints', headers=myHeaders, body = json.dumps(postData))
                if response.status != 201:
                  try:
                    ec2Client.delete_vpc_endpoints(VpcEndpointIds = [vpcEndpointId])
                    waitForEndpointDeletion(vpcEndpointId)
                  except Exception as e: print(str(e))
                  errorMessage = 'Unknown Error'
                  try: errorMessage = json.loads(response.data.decode())['message']
                  except: errorMessage = response.reason
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = errorMessage)
                  return
                dbsVpcEndpointId = json.loads(response.data.decode())['vpc_endpoint_id']
                waitForEndpoint(vpcEndpointId)
                ec2Client.modify_vpc_endpoint(VpcEndpointId=vpcEndpointId, PrivateDnsEnabled=True)
                waitForEndpoint(vpcEndpointId)
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, dbsVpcEndpointId)
                return
              if event['RequestType'] == 'Delete':
                response = http.request('DELETE', accountsAPIBaseURL + '/vpc-endpoints/' + event['PhysicalResourceId'], headers=myHeaders)
                if response.status != 200:
                  errorMessage = 'Unknown Error'
                  try: errorMessage = json.loads(response.data.decode())['message']
                  except: errorMessage = response.reason
                  print(errorMessage)
                else:
                  try:
                    awsVpcEndpointId = json.loads(response.data.decode())['aws_vpc_endpoint_id']
                    ec2Client.delete_vpc_endpoints(VpcEndpointIds = [awsVpcEndpointId])
                    waitForEndpointDeletion(awsVpcEndpointId)
                  except Exception as e: print(str(e))
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
          - { ServiceName: !FindInMap [PrivateLink, !Ref AWS::Region, workspace] }

# Workspace VPC endpoint log group
  WorkspaceVpcEndpointLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${WorkspaceVpcEndpointFn}
      RetentionInDays: 365

# VPC Endpoint for back-end secure cluster connectivity relay service
  BackendVpcEndpoint:
    Condition: EnablePrivateLink
    Type: Custom::BackendVpcEndpoint
    Properties:
      ServiceToken: !GetAtt BackendVpcEndpointFn.Arn
      DatabricksAccountId: !Ref AccountId
      Username: !Ref Username
      Password: !Ref Password
      EndpointName: !Sub ${AWS::StackName}_backendVpcEndpoint
      VpcId: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]
      Subnets: !If [HasExistingVPC, !Ref SubnetIDs, !Join [',', [!Ref privateSubnet1, !Ref privateSubnet2]]]
      SecurityGroups: !If [HasExistingVPC, !Ref SecurityGroupIDs, !Ref SecurityGroup]

# Custom resource lambda function definition for back-end VPC endpoint registration
  BackendVpcEndpointFn:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${ResourcePrefix}-BackendVpcEnpoint
      Description: The custom resource function to create a backend VPC endpoint
      Handler: index.lambda_handler
      MemorySize: 256
      Role: !GetAtt functionRole.Arn
      Runtime: python3.9
      Timeout: 600
      Code:
        ZipFile: !Sub 
          - |
            import cfnresponse, urllib3, json, boto3, time
            region = "${AWS::Region}"
            serviceName = "${ServiceName}"
            ec2Client = boto3.session.Session(region_name=region).client('ec2')
            def waitForEndpointDeletion(vpcEndpointId):
              while True:
                try:
                  status = ec2Client.describe_vpc_endpoints(Filters=[{'Name':'vpc-endpoint-id', 'Values': [vpcEndpointId]}])['VpcEndpoints'][0]['State']
                  time.sleep(5)
                except Exception as e: break
            def waitForEndpoint(vpcEndpointId):
              while True: # Wait for the endpoint to become available
                try:
                  status = ec2Client.describe_vpc_endpoints(Filters=[{'Name':'vpc-endpoint-id', 'Values': [vpcEndpointId]}])['VpcEndpoints'][0]['State']
                  if status == 'available': break
                except Exception as e: print(str(e))
                time.sleep(5)
            def checkForMissingProperty(properties, propertyName):
              if propertyName not in properties:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No " + propertyName + " property specified")
                return True
              return False
            def lambda_handler(event, context):
              for propertName in ('DatabricksAccountId', 'Username', 'Password'):
                if checkForMissingProperty(event['ResourceProperties'], propertName): return
              accountsAPIBaseURL = 'https://accounts.cloud.databricks.com/api/2.0/accounts/' + event['ResourceProperties']['DatabricksAccountId']
              myHeaders = urllib3.util.make_headers(basic_auth = event['ResourceProperties']['Username'] + ':' + event['ResourceProperties']['Password']) | {'Content-Type': 'application/json'}
              http = urllib3.PoolManager()
              if event['RequestType'] == 'Create':
                for propertName in ('EndpointName', 'VpcId', 'Subnets', 'SecurityGroups'):
                  if checkForMissingProperty(event['ResourceProperties'], propertName): return
                vpcEndpointId = None
                try: # Create the endpoint in the VPC
                  vpcEndpointId = ec2Client.create_vpc_endpoint(VpcEndpointType='Interface', VpcId=event['ResourceProperties']['VpcId'],
                    ServiceName=serviceName, SubnetIds=event['ResourceProperties']['Subnets'].split(","), SecurityGroupIds=[event['ResourceProperties']['SecurityGroups']],
                    TagSpecifications=[{"ResourceType": "vpc-endpoint","Tags": [{"Key": "Owner", "Value": "${ResourceOwner}"}]}])['VpcEndpoint']['VpcEndpointId']
                except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = str(e))
                  return
                postData = {"vpc_endpoint_name": event['ResourceProperties']['EndpointName'], "aws_vpc_endpoint_id": vpcEndpointId, "region": region}
                response = http.request('POST', accountsAPIBaseURL + '/vpc-endpoints', headers=myHeaders, body = json.dumps(postData))
                if response.status != 201:
                  try:
                    ec2Client.delete_vpc_endpoints(VpcEndpointIds = [vpcEndpointId])
                    waitForEndpointDeletion(vpcEndpointId)
                  except Exception as e: print(str(e))
                  errorMessage = 'Unknown Error'
                  try: errorMessage = json.loads(response.data.decode())['message']
                  except: errorMessage = response.reason
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = errorMessage)
                  return
                dbsVpcEndpointId = json.loads(response.data.decode())['vpc_endpoint_id']
                waitForEndpoint(vpcEndpointId)
                ec2Client.modify_vpc_endpoint(VpcEndpointId=vpcEndpointId, PrivateDnsEnabled=True)
                waitForEndpoint(vpcEndpointId)
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, dbsVpcEndpointId)
                return
              if event['RequestType'] == 'Delete':
                response = http.request('DELETE', accountsAPIBaseURL + '/vpc-endpoints/' + event['PhysicalResourceId'], headers=myHeaders)
                if response.status != 200:
                  errorMessage = 'Unknown Error'
                  try: errorMessage = json.loads(response.data.decode())['message']
                  except: errorMessage = response.reason
                  print(errorMessage)
                else:
                  try:
                    awsVpcEndpointId = json.loads(response.data.decode())['aws_vpc_endpoint_id']
                    ec2Client.delete_vpc_endpoints(VpcEndpointIds = [awsVpcEndpointId])
                    waitForEndpointDeletion(awsVpcEndpointId)
                  except Exception as e: print(str(e))
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
          - { ServiceName: !FindInMap [PrivateLink, !Ref AWS::Region, backend] }

  # The log group on CloudWatch logs
  BackendVpcEnpointLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${BackendVpcEndpointFn}
      RetentionInDays: 365
  
 # Creates a private access settings object, which specifies how the workspace is accessed over AWS PrivateLink:
 # https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-private-access-settings 
  PrivateAccessConfiguration:
    Condition: EnablePrivateLink
    Type: Custom::PrivateAccessConfiguration
    Properties:
      ServiceToken: !GetAtt PrivateAccessConfigurationFn.Arn
      DatabricksAccountId: !Ref AccountId
      Username: !Ref Username
      Password: !Ref Password
      PrivateAccessSettingsName:  !Sub ${AWS::StackName}_privateAccessSettings
      PublicAccessEnabled: !If [ PublicAccessForPrivateLink, true, false ]
      AllowedVpcEndpoints:
        - !Ref WorkspaceVpcEndpoint

# custom resource lambda function definition
# https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-private-access-settings
  PrivateAccessConfigurationFn:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: PrivateAccessConfiguration
      Description: The custom resource function to create a private access configuration for a Databricks workspace
      Handler: index.lambda_handler
      MemorySize: 256
      Role: !GetAtt functionRole.Arn
      Runtime: python3.9
      Timeout: 25
      Code:
        ZipFile: !Sub |
          import cfnresponse
          import urllib3
          import json
          def checkForMissingProperty(properties, propertyName):
            if propertyName not in properties:
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No " + propertyName + " property specified")
              return True
            return False
          def lambda_handler(event, context):
            for propertName in ('DatabricksAccountId', 'Username', 'Password'):
              if checkForMissingProperty(event['ResourceProperties'], propertName): return
            accountsAPIBaseURL = 'https://accounts.cloud.databricks.com/api/2.0/accounts/' + event['ResourceProperties']['DatabricksAccountId']
            myHeaders = urllib3.util.make_headers(basic_auth = event['ResourceProperties']['Username'] + ':' + event['ResourceProperties']['Password']) | {'Content-Type': 'application/json'}
            http = urllib3.PoolManager()
            if event['RequestType'] == 'Create':
              if 'PrivateAccessSettingsName' not in event['ResourceProperties']:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No PrivateAccessSettingsName property specified")
                return
              if 'PublicAccessEnabled' not in event['ResourceProperties']:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No PublicAccessEnabled property specified")
                return
              if ('AllowedVpcEndpoints' not in event['ResourceProperties']) or type(event['ResourceProperties']['AllowedVpcEndpoints']) != list:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No AllowedVpcEndpoints property specified")
                return
              postData = {"private_access_settings_name": event['ResourceProperties']['PrivateAccessSettingsName'],
                "region": "${AWS::Region}",
                "public_access_enabled": event['ResourceProperties']['PublicAccessEnabled'],
                "private_access_level": "ENDPOINT",
                "allowed_vpc_endpoint_ids": event['ResourceProperties']['AllowedVpcEndpoints']
              }
              response = http.request('POST', accountsAPIBaseURL + '/private-access-settings', headers=myHeaders, body = json.dumps(postData))
              if response.status != 201:
                errorMessage = 'Unknown Error'
                try: errorMessage = json.loads(response.data.decode())['message']
                except: errorMessage = response.reason
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = errorMessage)
                return
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, json.loads(response.data.decode())['private_access_settings_id'])
              return
            if event['RequestType'] == 'Delete':
              response = http.request('DELETE', accountsAPIBaseURL + '/private-access-settings/' + event['PhysicalResourceId'], headers=myHeaders)
              if response.status != 200:
                errorMessage = 'Unknown Error'
                try: errorMessage = json.loads(response.data.decode())['message']
                except: errorMessage = response.reason
                print(errorMessage)
            cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
      Tags:
        - Key: Owner
          Value: !Ref ResourceOwner

  # The log group on CloudWatch logs
  PrivateAccessConfigurationLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${PrivateAccessConfigurationFn}
      RetentionInDays: 365

# Databricks API for workspace creation, https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-workspace
  createWorkspace:
    Type: Custom::DatabricksWorkspace
    Properties:
      ServiceToken: !GetAtt 'WorkspaceFn.Arn'
      DatabricksAccountId: !Ref AccountId
      WorkspaceName: !Sub ${AWS::StackName}_workspace
      CredentialsId: !GetAtt createCredentials.CredentialsId
      StorageId: !GetAtt createStorageConfiguration.StorageConfigId
      Username: !Ref Username
      Password: !Ref Password
      NetworkId: !GetAtt createNetworks.NetworkId
      ManagedServicesKeyId: !If [CMKEncrypted, !GetAtt 'createCustomerManagedKey.CustomerManagedKeyId', '']
      PrivateAccessId: !If [ EnablePrivateLink, !Ref PrivateAccessConfiguration, !Ref AWS::NoValue ]

# Custom resource definition for https://docs.databricks.com/dev-tools/api/latest/account.html#operation/create-workspace
  WorkspaceFn:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: Workspace
      Description: The custom resource function to create a Databricks workspace
      Handler: index.lambda_handler
      MemorySize: 256
      Role: !GetAtt functionRole.Arn
      Runtime: python3.9
      Timeout: 600
      Code:
        ZipFile: !Sub |
          import cfnresponse, urllib3, json, time
          def checkForMissingProperty(properties, propertyName):
            if propertyName not in properties:
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = "No " + propertyName + " property specified")
              return True
            return False
          def lambda_handler(event, context):
            for propertName in ('DatabricksAccountId', 'Username', 'Password'):
              if checkForMissingProperty(event['ResourceProperties'], propertName): return
            accountsAPIBaseURL = 'https://accounts.cloud.databricks.com/api/2.0/accounts/' + event['ResourceProperties']['DatabricksAccountId']
            myHeaders = urllib3.util.make_headers(basic_auth = event['ResourceProperties']['Username'] + ':' + event['ResourceProperties']['Password']) | {'Content-Type': 'application/json'}
            http = urllib3.PoolManager()
            if event['RequestType'] == 'Create':
              for propertName in ('WorkspaceName', 'CredentialsId', 'StorageId', 'NetworkId'):
                if checkForMissingProperty(event['ResourceProperties'], propertName): return
              postData = {
                "workspace_name": event['ResourceProperties']['WorkspaceName'],
                "aws_region": "${AWS::Region}",
                "credentials_id": event['ResourceProperties']['CredentialsId'],
                "storage_configuration_id": event['ResourceProperties']['StorageId'],
                "network_id": event['ResourceProperties']['NetworkId']
              }
              if ('ManagedServicesKeyId' in event['ResourceProperties']) and (event['ResourceProperties']['ManagedServicesKeyId'] is not None) and (len(event['ResourceProperties']['ManagedServicesKeyId']) > 0):
                postData["managed_services_customer_managed_key_id"] = event['ResourceProperties']['ManagedServicesKeyId']
              if ('StorageKeyId' in event['ResourceProperties']) and (event['ResourceProperties']['StorageKeyId'] is not None) and (len(event['ResourceProperties']['StorageKeyId']) > 0):
                postData["storage_customer_managed_key_id"] = event['ResourceProperties']['StorageKeyId']
              if ('PrivateAccessId' in event['ResourceProperties']) and (event['ResourceProperties']['PrivateAccessId'] is not None) and (len(event['ResourceProperties']['PrivateAccessId']) > 0):
                postData["private_access_settings_id"] = event['ResourceProperties']['PrivateAccessId']
              response = http.request('POST', accountsAPIBaseURL + '/workspaces', headers=myHeaders, body = json.dumps(postData))
              if response.status != 201:
                errorMessage = 'Unknown Error'
                try: errorMessage = json.loads(response.data.decode())['message']
                except: errorMessage = response.reason
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason = errorMessage)
                return
              workspaceId = str(json.loads(response.data.decode())['workspace_id'])
              while True:
                time.sleep(5)
                try:
                  response = http.request('GET', accountsAPIBaseURL + '/workspaces/' + workspaceId, headers=myHeaders)
                  if json.loads(response.data.decode())['workspace_status'] != 'PROVISIONING': break
                except Exception as e:
                  print(str(e))
                  break
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, workspaceId)
              return
            if event['RequestType'] == 'Delete':
              response = http.request('DELETE', accountsAPIBaseURL + '/workspaces/' + event['PhysicalResourceId'], headers=myHeaders)
              if response.status != 200:
                errorMessage = 'Unknown Error'
                try: errorMessage = json.loads(response.data.decode())['message']
                except: errorMessage = response.reason
                print(errorMessage)
            cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
      Tags:
        - Key: Owner
          Value: !Ref ResourceOwner

  # The log group on CloudWatch logs
  WorkspaceLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${WorkspaceFn}
      RetentionInDays: 365

 # Update Security groups with cross-account IAM role if VPC already created. 
  updateIAMSecurityGroupIds:
    DependsOn: updateIAMRoleFunction
    Type: Custom::UpdateRoleAssumePolicy
    Properties:
      ServiceToken: !GetAtt 'updateIAMRoleFunction.Arn'
      # Hard Coded
      role_name: !If [CreateCrossAccountIAMRole, !Ref NewCrossAccountIAMRoleName, !Ref ExistingCrossAccountIAMRoleName]
      aws_region: !Ref AWSRegion
      accountId: !Ref AWS::AccountId
      security_group_ids: !Ref SecurityGroupIDs
      VPCID: !If [HasExistingVPC, !Ref VPCID, !Ref customerManagedVPC]

# Customer managed Keys - Update Storage policy for S3 and EBS volumes 
  updateCustomManagedKeys:
    Condition: CMKEncrypted
    DependsOn: updateKMSkeysFunction
    Type: Custom::updateCustomManagedKeys
    Properties:
      ServiceToken: !GetAtt 'updateKMSkeysFunction.Arn'
      key_id: !GetAtt CustomerManagedKey.Arn
      arn_credentials: !If [CreateCrossAccountIAMRole, !GetAtt 'accessRoleCustomerManagedVPC.Arn', !Ref ExistingCrossAccountIAMRoleARN]
      use_cases: !Ref KeyUseCases
      reuse_key_for_cluster_volumes: !Ref KeyReuseForClusterVolumes

# Databricks main Lambda for all E2 objects and workspace creation
  databricksApiFunction:
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Databricks account API.
      Handler: rest_client.handler
      Runtime: python3.8
      Role: !GetAtt 'functionRole.Arn'
      Timeout: 900
      Code:
        S3Bucket: !Ref 'LambdaZipsBucket'
        S3Key: !Sub '${QSS3KeyPrefix}functions/packages/lambda.zip'

  updateIAMRoleFunction:
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Update IAM role policy document using the list of security group IDs.
      Handler: update_custommanagedvpc_iam_role.handler
      Runtime: python3.8
      Role: !GetAtt 'functionRole.Arn'
      Timeout: 60
      Code:
        S3Bucket: !Ref 'LambdaZipsBucket'
        S3Key: !Sub '${QSS3KeyPrefix}functions/packages/lambda.zip'

  updateKMSkeysFunction:
    Condition: CMKEncrypted
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Update CMK policy document for storage.
      Handler: update_custommanaged_cmk_policy.handler
      Runtime: python3.8
      Role: !GetAtt 'functionRole.Arn'
      Timeout: 60
      Code:
        S3Bucket: !Ref 'LambdaZipsBucket'
        S3Key: !Sub '${QSS3KeyPrefix}functions/packages/lambda.zip'

# IAM Role for lambda function execution
  functionRole:
    Type: AWS::IAM::Role
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - EIAMPolicyWildcardResource
          ignore_reasons:
            EIAMPolicyWildcardResource: "Need to manage databricks workspaces"
    Properties:
      Description: Basic execution role for the CloudFormation custom resource Lambda functions using the Databricks Account API
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole 
      Policies:
        - PolicyName: kmsUpdateRole
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action: 
                  - 'kms:GetKeyPolicy'
                  - 'kms:PutKeyPolicy'
                Resource: '*' 
        - PolicyName: iamUpdateRole
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action: 
                  - 'iam:PutRolePolicy'
                Resource: '*'
        - PolicyName: vpcEndpointManagement
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Sid: AllowVPCEndpointManagement
                Effect: Allow
                Action:
                  - ec2:CreateVpcEndpoint
                  - ec2:ModifyVpcEndpoint
                  - ec2:DescribeVpcEndpoints
                  - ec2:DeleteVpcEndpoints
                Resource: '*'
      Tags:
        -
          Key: Name
          Value: !Sub '${TagValue}-IAMRole'

# Resources to stage lambda.zip file
  LambdaZipsBucket:
    Type: AWS::S3::Bucket
  CopyZips:
    Type: Custom::CopyZips
    Properties:
      ServiceToken: !GetAtt 'CopyZipsFunction.Arn'
      DestBucket: !Ref 'LambdaZipsBucket'
      SourceBucket: !Ref 'QSS3BucketName'
      Prefix: !Ref 'QSS3KeyPrefix'
      Objects:
        - functions/packages/lambda.zip
  CopyZipsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: lambda-copier
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:DeleteObject
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${LambdaZipsBucket}/${QSS3KeyPrefix}*' 
      Tags:
        -
          Key: Name
          Value: !Sub '${TagValue}-IAMRole'

# Copy zips from Quick Start bucket over to data plane bucket
  CopyZipsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Copies objects from an S3 bucket to another destination.
      Handler: index.handler
      Runtime: python3.8
      Role: !GetAtt 'CopyZipsRole.Arn'
      Timeout: 240
      Code:
        ZipFile: |
          import json
          import logging
          import threading
          import boto3
          import cfnresponse
          def copy_objects(source_bucket, dest_bucket, prefix, objects):
              s3 = boto3.client('s3')
              for o in objects:
                  key = prefix + o
                  copy_source = {
                      'Bucket': source_bucket,
                      'Key': key
                  }
                  print('copy_source: %s' % copy_source)
                  print('dest_bucket = %s'%dest_bucket)
                  print('key = %s' %key)
                  s3.copy_object(CopySource=copy_source, Bucket=dest_bucket,
                        Key=key)
          def delete_objects(bucket, prefix, objects):
              s3 = boto3.client('s3')
              objects = {'Objects': [{'Key': prefix + o} for o in objects]}
              s3.delete_objects(Bucket=bucket, Delete=objects)
          def timeout(event, context):
              logging.error('Execution is about to time out, sending failure response to CloudFormation')
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None)
          def handler(event, context):
              # make sure we send a failure to CloudFormation if the function
              # is going to timeout
              timer = threading.Timer((context.get_remaining_time_in_millis()
                        / 1000.00) - 0.5, timeout, args=[event, context])
              timer.start()
              print('Received event: %s' % json.dumps(event))
              status = cfnresponse.SUCCESS
              try:
                  source_bucket = event['ResourceProperties']['SourceBucket']
                  dest_bucket = event['ResourceProperties']['DestBucket']
                  prefix = event['ResourceProperties']['Prefix']
                  objects = event['ResourceProperties']['Objects']
                  if event['RequestType'] == 'Delete':
                      delete_objects(dest_bucket, prefix, objects)
                  else:
                      copy_objects(source_bucket, dest_bucket, prefix, objects)
              except Exception as e:
                  logging.error('Exception: %s' % e, exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  timer.cancel()
                  cfnresponse.send(event, context, status, {}, None)
                  