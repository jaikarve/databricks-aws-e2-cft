AWSTemplateFormatVersion: 2010-09-09
Description: >-
  This template creates Databricks workspace resources in your AWS account using the API account. The API account is required if you want to use either customer managed VPCs or customer managed keys for notebooks. For feature availability, contact your Databricks representative. (qs-1r0odiedc)
Metadata:
  QuickStartDocumentation:
    EntrypointName: "Parameters for deploying a workspace and creating a cross-account IAM role"
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Workspace configuration"
        Parameters:
          - AccountId
          - Username
          - Password
          - PricingTier
          - DeploymentName
          - AWSRegion
          - HIPAAparm
      - Label:
          default: "IAM role and S3 bucket configuration"
        Parameters:
          - TagValue
          - IAMRoleARN
          - BucketName
      - Label:
          default: "(Optional) Customer managed key configuration for notebooks (requires the enterprise tier)"
        Parameters:
          - KeyArn
          - KeyAlias
          - KeyUseCases
          - KeyReuseForClusterVolumes
      - Label:
          default: "Quick Start configuration"
        Parameters:
          - QSS3BucketName
          - QSS3KeyPrefix
    ParameterLabels:
      AccountId:
        default: Databricks account ID
      Username:
        default: Workspace account email
      Password:
        default: Workspace account password
      PricingTier:
        default: Pricing tier of the workspace
      DeploymentName:
        default: Workspace deployment name
      AWSRegion:
        default: AWS Region of the Databricks workspace 
      HIPAAparm:
        default: HIPAA tier account
      TagValue:
        default: IAM role tag
      IAMRoleARN:
        default: Cross-account IAM role ARN
      BucketName:
        default: Root S3 bucket name
      KeyArn:
        default: ARN for the customer managed AWS KMS key
      KeyAlias:
        default: Alias for the customer managed AWS KMS key
      KeyUseCases:
        default: Use case for which to use the key
      KeyReuseForClusterVolumes:
        default: Encrypt cluster EBS volumes
      QSS3BucketName:
        default: Quick Start S3 bucket name
      QSS3KeyPrefix:
        default: Quick Start S3 key prefix

Outputs:
  SubnetIDs:
    Description: Comma separated list of subnets
    Value: !Join
      - ','
      - - !Ref privateSubnet1
        - !Ref privateSubnet2
  S3BucketName:
    Description: Name of the S3 root bucket.
    Value: !Ref assetsS3Bucket

Parameters:
  AccountId:
    Description: "Account must use the E2 version of the platform. For more information, see https://docs.databricks.com/getting-started/overview.html#e2-architecture." 
    AllowedPattern: '^[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}$'
    MinLength: '36'
    Type: String
    Default: aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee
  Username:
    Description: "Account email for authenticating the REST API. Note that this value is case sensitive."
    AllowedPattern: '^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+$'
    ConstraintDescription: Must be a valid email format.
    MinLength: '8'
    Type: String
  Password:
    Description: "Account password for authenticating the REST API. The minimum length is 8 alphanumeric characters."
    MinLength: '8'
    NoEcho: 'true'
    Type: String
  PricingTier:
    Description: "If you do not provide this, the API defaults to the highest pricing tier. For more information, see https://databricks.com/product/aws-pricing."
    AllowedValues:
       - STANDARD
       - PREMIUM 
       - ENTERPRISE
       - '' 
    Type: String
    Default: ''
  DeploymentName:
    Description: "Choose this value carefully. The deployment name defines part of the workspace subdomain (e.g., workspace-deployment-name.cloud.databricks.com). This value must be unique across all deployments in all AWS Regions. It cannot start or end with a hyphen. If your account has a deployment-name prefix, add the prefix followed by a hyphen. For more information, see https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-5-create-the-workspace."
    AllowedPattern: '^(([a-z0-9][a-z0-9-]*[a-z0-9])|([a-z0-9]))$'
    ConstraintDescription: You must enter a valid subdomain for the workspace.
    Type: String
  AWSRegion:
    Description: "AWS Region where the workspace is created. Note that customer managed keys to encrypt notebooks are not supported in the us-west-1 Region."
    MinLength: '9'
    AllowedValues:
       - ap-northeast-1
       - ap-south-1
       - ap-southeast-2
       - ca-central-1
       - eu-central-1
       - eu-west-1
       - eu-west-2
       - us-east-1
       - us-east-2
       - us-west-1
       - us-west-2
    Type: String
  HIPAAparm:
    Description: 'Entering "Yes" creates a template for creating clusters in the HIPAA account.'
    AllowedValues:
       - 'Yes'
       - 'No'
    Default: 'No'
    Type: String
  TagValue:
    Description: "All new AWS objects get a tag with the key name. Enter a value to identify all new AWS objects that this template creates. For more information, see https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html."
    MinLength: '1'
    Type: String
    Default: databricks-quickstart-cloud-formation
  IAMRoleARN: 
    Description: "Enter a unique cross-account IAM role name. For more information, see https://docs.aws.amazon.com/IAM/latest/APIReference/API_CreateRole.html."
    Type: String
    MinLength: '1'
  BucketName:
    Description: "Name of your S3 root bucket. Enter only alphanumeric characters. For more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html."
    AllowedPattern: '(?=^.{3,63}$)(?!xn--)([a-z0-9](?:[a-z0-9-]*)[a-z0-9])$'
    MinLength: '3'
    MaxLength: '63'
    Type: String
    ConstraintDescription: Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).
  KeyArn:
    Description: "AWS KMS key ARN to encrypt and decrypt workspace notebooks in the control plane. Only enter a value if you use the customer managed key for notebooks. For more information, see https://docs.databricks.com/security/keys/customer-managed-keys-notebook-aws.html."
    Type: String
    Default: ''
  KeyAlias:
    Description: "(Optional) AWS KMS key alias."
    Type: String
    Default: ''
  KeyUseCases:
    Description: "Configures customer managed encryption keys. Acceptable values are MANAGED_SERVICES, STORAGE, or BOTH. For more information, see https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-5-configure-customer-managed-keys-optional."
    Type: String 
  KeyReuseForClusterVolumes:
    Description: 'Only enter a value if the use case is STORAGE or BOTH. Acceptable values are "True" and "False."'
    Type: String
  QSS3BucketName:
    Description: "S3 bucket for Quick Start assets. Use this if you want to customize the Quick Start. The bucket name can include numbers, lowercase letters, uppercase letters, and hyphens, but it cannot start or end with a hyphen (-)."
    AllowedPattern: '^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$'
    Default: aws-quickstart
    Type: String
    MinLength: '3'
    MaxLength: '63'
    ConstraintDescription: Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).
  QSS3KeyPrefix:
    Description: "S3 key prefix to simulate a directory for your Quick Start assets. Use this if you want to customize the Quick Start. The prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slashes (/). For more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html."
    AllowedPattern: '^[0-9a-zA-Z-/]*$'
    Type: String
    Default: quickstart-databricks-unified-data-analytics-platform/
  

Conditions:
# Set condition when AWS KMS key ID is provided by the user. 
  IsKMSKeyProvided: !Not [!Equals [!Ref KeyArn, '']]
# Test for STORAGE CMK use case
  IsClusterVolumeSet: !Equals [!Ref KeyReuseForClusterVolumes, 'True']

Rules:
# 1. Validate the selected Region from drop-down matches the Region from the Console
  RunningTemplateFromDifferentRegionThanDropDown:
    Assertions:
    - Assert: !Equals [!Ref AWSRegion, !Ref 'AWS::Region']
      AssertDescription: Region from the AWS Management Console must match the selected Region from the drop-down menu.

# 2. HIPAA supported regions, if HIPAA flaf is set
  SupportedHipaaRegions:
    RuleCondition: !Equals [!Ref HIPAAparm, 'Yes']
    Assertions:
    - Assert: !Contains [['us-east-1', 'us-east-2', 'ca-central-1'], !Ref AWSRegion]
      AssertDescription: Creates a workspace for only HIPAA tier accounts in the us-east-1, us-east-2, and ca-central-1 Regions.

# 4. Optional Section. Ensure the KeyARN is provided when a use case is specified.
  KeyUseCases1:
    RuleCondition: !Not [!Equals [!Ref KeyArn, '']]
    Assertions:
    - Assert: !Contains [['MANAGED_SERVICES', 'STORAGE', 'BOTH'],!Ref KeyUseCases]
      AssertDescription: Acceptable values are MANAGED_SERVICES, STORAGE, or BOTH when you provide a role ARN.
    
  KeyUseCases2:
    RuleCondition: !Or
      - !Equals [!Ref KeyUseCases, 'STORAGE']
      - !Equals [!Ref KeyUseCases, 'BOTH']
    Assertions:
    - Assert: !Contains [['True', 'False'], !Ref KeyReuseForClusterVolumes]
      AssertDescription: 'Acceptable values are "True" and "False" when the use case is either STORAGE or BOTH.' 
 
  KeyUseCases3:
    RuleCondition: !Equals [!Ref KeyUseCases, 'MANAGED_SERVICES']
    Assertions: 
    - Assert: !Equals [!Ref KeyReuseForClusterVolumes, '']
      AssertDescription: Value must be null if MANAGED_SERVICES is specified.

# 5. Assertion rule to prevent changing the QuickStart Bucket name and Prefix parameter
# ***********************************************************************************************************************
# This rule must be COMMENTED if it is intended to clone the git repo to make modifications prior to promote the changes
# ***********************************************************************************************************************
  AWSQuickStartGitParametersSettings:
    Assertions:
    - Assert: !Equals ['aws-quickstart', !Ref QSS3BucketName]
      AssertDescription: 'QSS3BucketName must be aws-quickstart.'
    - Assert: !Equals ['quickstart-databricks-unified-data-analytics-platform/', !Ref QSS3KeyPrefix]
      AssertDescription: 'QSS3KeyPrefix must be quickstart-databricks-unified-data-analytics-platform/.'
        
Resources:
  customerManagedVPC:
    Type: 'AWS::EC2::VPC'
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: 'true'
      EnableDnsSupport: 'true'
  privateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref customerManagedVPC
      AvailabilityZone: "us-east-1a"
      CidrBlock: 10.0.0.0/24
  privateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.1.0/24
      VpcId: !Ref customerManagedVPC
      AvailabilityZone: "us-east-1b"
  publicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.2.0/24
      VpcId: !Ref customerManagedVPC
      AvailabilityZone: "us-east-1c"
      MapPublicIpOnLaunch: 'true'
  internetGateway:
    Type: AWS::EC2::InternetGateway
  internetGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      InternetGatewayId: !Ref internetGateway
      VpcId: !Ref customerManagedVPC
  NatGateway1EIP:
    Type: AWS::EC2::EIP
    DependsOn: internetGatewayAttachment
    Properties:
      Domain: vpc
  NatGateway1:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt NatGateway1EIP.AllocationId
      SubnetId: !Ref publicSubnet
  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref customerManagedVPC
  DefaultPublicRoute:
    Type: AWS::EC2::Route
    DependsOn: internetGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref internetGateway
  PublicSubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PublicRouteTable
      SubnetId: !Ref publicSubnet
  PrivateRouteTable1:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref customerManagedVPC
  DefaultPrivateRoute1:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NatGateway1
  PrivateSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      SubnetId: !Ref privateSubnet1
  PrivateSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      SubnetId: !Ref privateSubnet2
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      VpcId: !Ref customerManagedVPC
      GroupDescription: Security group for Databricks E2 deployment
  SecurityGroupIngressTcp:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref SecurityGroup
      SourceSecurityGroupId: !Ref SecurityGroup
      IpProtocol: tcp
      FromPort: 0
      ToPort: 65535
  SecurityGroupIngressUdp:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref SecurityGroup
      SourceSecurityGroupId: !Ref SecurityGroup
      IpProtocol: udp
      FromPort: 0
      ToPort: 65535
  SecurityGroupEgressAll:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      GroupId: !Ref SecurityGroup
      CidrIp: 0.0.0.0/0
      IpProtocol: -1
  NetworkAcl:
    Type: AWS::EC2::NetworkAcl
    Properties:
      VpcId: !Ref customerManagedVPC
  NetworkAclEntryInboundAllow:
    Type: AWS::EC2::NetworkAclEntry
    Properties:
      Egress: "false"
      CidrBlock: 0.0.0.0/0
      Protocol: -1
      RuleNumber: 100
      RuleAction: allow
      NetworkAclId: !Ref NetworkAcl
  NetworkAclEntryOutboundAllow:
    Type: AWS::EC2::NetworkAclEntry
    Properties:
      Egress: "true"
      CidrBlock: 0.0.0.0/0
      Protocol: -1
      RuleNumber: 100
      RuleAction: allow
      NetworkAclId: !Ref NetworkAcl
  SubnetNetworkAclNAT:
    Type: AWS::EC2::SubnetNetworkAclAssociation
    Properties:
      NetworkAclId: !Ref NetworkAcl
      SubnetId: !Ref publicSubnet
  SubnetNetworkAclPrivateSubnet1:
    Type: AWS::EC2::SubnetNetworkAclAssociation
    Properties:
      NetworkAclId: !Ref NetworkAcl
      SubnetId: !Ref privateSubnet1
  SubnetNetworkAclPrivateSubnet2:
    Type: AWS::EC2::SubnetNetworkAclAssociation
    Properties:
      NetworkAclId: !Ref NetworkAcl
      SubnetId: !Ref privateSubnet2
  #VPCEndpointS3:
   # Type: AWS::EC2::VPCEndpoint
    #Properties:
     # RouteTableIds:
      #  - !Ref PrivateRouteTable1
      #ServiceName: 'com.amazonaws.us-east-1.s3'
      #VpcId: !Ref customerManagedVPC
  #VPCEndpointSTS:
   # Type: AWS::EC2::VPCEndpoint
    #Properties:
     # VpcEndpointType: Interface
      #PrivateDnsEnabled: true
      #SecurityGroupIds:
       # - !Ref SecurityGroup
      #SubnetIds:
       # - !Ref privateSubnet1
        #- !Ref privateSubnet2
      #ServiceName: 'com.amazonaws.sts'
      #VpcId: !Ref customerManagedVPC
  #VPCEndpointKinesis:
   # Type: AWS::EC2::VPCEndpoint
    #Properties:
     # VpcEndpointType: Interface
      #PrivateDnsEnabled: true
      #SecurityGroupIds:
       # - !Ref SecurityGroup
      #SubnetIds:
       # - !Ref privateSubnet1
        #- !Ref privateSubnet2
      #ServiceName: 'com.amazonaws.us-east-1.kinesis'
      #VpcId: !Ref customerManagedVPC
  #VPCEndpointWorkspace:
   # Type: AWS::EC2::VPCEndpoint
    #Properties:
     # VpcEndpointType: Interface
      #PrivateDnsEnabled: true
      #SecurityGroupIds:
       # - !Ref VPCEndpointSG
      #SubnetIds:
       # - !Ref VPCEndpointSubnet
      #ServiceName: 'com.amazonaws.vpce.us-east-1.vpce-svc-09143d1e626de2f04'
      #VpcId: !Ref customerManagedVPC
  #VPCEndpointSCC:
   # Type: AWS::EC2::VPCEndpoint
    #Properties:
     # VpcEndpointType: Interface
      #PrivateDnsEnabled: true
      #SecurityGroupIds:
       # - !Ref VPCEndpointSG
      #SubnetIds:
       # - !Ref VPCEndpointSubnet
      #ServiceName: 'com.amazonaws.vpce.us-east-1.vpce-svc-00018a8c3ff62ffdf'
      #VpcId: !Ref customerManagedVPC
  #VPCEndpointSubnet:
   # Type: AWS::EC2::Subnet
    #Properties:
     # CidrBlock: 10.0.3.0/24
      #VpcId: !Ref customerManagedVPC
      #AvailabilityZone: "us-east-1d"
  #VPCEndpointSG:
   # Type: AWS::EC2::SecurityGroup
    #Properties:
     # GroupDescription: Allow VPC endpoint services to reach control plane infra
      #VpcId: !Ref customerManagedVPC
  #SecurityGroupEgressRest:
   # Type: AWS::EC2::SecurityGroupEgress
    #Properties:
     # DestinationSecurityGroupId: !Ref VPCEndpointSG
      #GroupId: !Ref VPCEndpointSG
      #IpProtocol: tcp
      #FromPort: 443
      #ToPort: 443
  #SecurityGroupEgressPL:
   # Type: AWS::EC2::SecurityGroupEgress
    #Properties:
     # DestinationSecurityGroupId: !Ref VPCEndpointSG
      #GroupId: !Ref VPCEndpointSG
      #IpProtocol: tcp
      #FromPort: 6666
      #ToPort: 6666
  #SecurityGroupIngressRest:
   # Type: AWS::EC2::SecurityGroupIngress
    #Properties:
     # SourceSecurityGroupId: !Ref VPCEndpointSG
      #GroupId: !Ref VPCEndpointSG
      #IpProtocol: tcp
      #FromPort: 443
      #ToPort: 443
  #SecurityGroupIngressPL:
   # Type: AWS::EC2::SecurityGroupIngress
    #Properties:
     # SourceSecurityGroupId: !Ref VPCEndpointSG
      #GroupId: !Ref VPCEndpointSG
      #IpProtocol: tcp
      #FromPort: 6666
      #ToPort: 6666
  
  # Cross-account access role for Databricks-created and managed VPC  
  # Cross-account access role for customer managed VPC
  # S3 root bucket requirements
  assetsS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls       : true
        BlockPublicPolicy     : true
        IgnorePublicAcls      : true
        RestrictPublicBuckets : true
      LifecycleConfiguration:
        Rules: 
          - Id: DeleteContentAfter5days
            Status: Enabled
            NoncurrentVersionExpirationInDays: 5
      VersioningConfiguration: 
        Status: Enabled
  bucketPolicy:
    Type: 'AWS::S3::BucketPolicy'
    Properties: 
      PolicyDocument:
        Id: MyPolicy
        Version: 2012-10-17
        Statement:
          - Sid: Grant Databricks Access
            Effect: Allow
            Principal:
              AWS: arn:aws:iam::414351767826:root
            Action:
              - 's3:GetObject'
              - 's3:GetObjectVersion'
              - 's3:PutObject'
              - 's3:DeleteObject'
              - 's3:ListBucket'
              - 's3:GetBucketLocation'
            Resource:
              - !Sub 'arn:${AWS::Partition}:s3:::${assetsS3Bucket}/*'
              - !Sub 'arn:${AWS::Partition}:s3:::${assetsS3Bucket}'
      Bucket: !Ref assetsS3Bucket 

  # Databricks API for configuring notebook encryption with a customer managed AWS KMS, if provided
  createCustomerManagedKey:
    Condition: IsKMSKeyProvided
    DependsOn: updateCustomManagedKeys
    Type: Custom::CreateCustomerManagedKey
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_CUSTOMER_MANAGED_KEY'
      accountId: !Ref AccountId
      key_arn: !Ref KeyArn
      key_alias: !Ref KeyAlias
      use_cases: !Ref KeyUseCases
      reuse_key_for_cluster_volumes: !Ref KeyReuseForClusterVolumes
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for workspace credentials
  createCredentials:
    Type: Custom::CreateCredentials
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_CREDENTIALS'
      accountId: !Ref AccountId
      credentials_name: !Join
        - '-'
        - - !Ref DeploymentName
          - 'credentials'
      role_arn: !Ref IAMRoleARN
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for workspace storage configuration
  createStorageConfiguration:
    Type: Custom::CreateStorageConfigurations
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_STORAGE_CONFIGURATIONS'
      accountId: !Ref AccountId
      storage_config_name: !Join
        - '-'
        - - !Ref 'DeploymentName'
          - 'storage'
      s3bucket_name: !Ref assetsS3Bucket
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for customer managed VPC
  createNetworks:
    DependsOn: createStorageConfiguration
    Type: Custom::createNetworks
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_NETWORKS'
      accountId: !Ref AccountId
      network_name: !Join
        - '-'
        - - !Ref 'DeploymentName'
          - 'network'
      vpc_id: !Ref customerManagedVPC
      subnet_ids: !Join
        - ','
        - - !Ref privateSubnet1
          - !Ref privateSubnet2
          - !Ref publicSubnet
      security_group_ids: !Ref SecurityGroup
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for workspace creation
  createWorkspace:
    Type: Custom::CreateWorkspace
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_WORKSPACES'
      accountId: !Ref AccountId
      workspace_name: !Join
        - '-'
        - - !Ref 'DeploymentName'
          - 'workspace'
      deployment_name: !Ref 'DeploymentName'
      aws_region: !Ref AWSRegion
      credentials_id: !GetAtt createCredentials.CredentialsId
      storage_config_id: !GetAtt createStorageConfiguration.StorageConfigId
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      network_id: !GetAtt createNetworks.NetworkId
      customer_managed_key_id: !If [IsKMSKeyProvided, !GetAtt 'createCustomerManagedKey.CustomerManagedKeyId', '']
      pricing_tier: !Ref PricingTier
      hipaa_parm: !Ref HIPAAparm
      customer_name: ''
      authoritative_user_email: ''
      authoritative_user_full_name: ''
      user_agent: 'databricks-CloudFormation-provider'
  
  # Customer managed VPC - Create databricks-cross-account-iam-role-policy-sg policy 
  updateIAMSecurityGroupIds:
    DependsOn: updateIAMRoleFunction
    Type: Custom::UpdateRoleAssumePolicy
    Properties:
      ServiceToken: !GetAtt 'updateIAMRoleFunction.Arn'
      # HARD CODED IAM ROLE NAME
      role_name: "DBCrossAccountIAMRole"
      aws_region: !Ref AWSRegion
      accountId: !Ref AWS::AccountId
      security_group_ids: !Ref SecurityGroup
      VPCID: !Ref customerManagedVPC

  # Customer managed Keys - Update Storage policy for S3 and EBS volumes 
  updateCustomManagedKeys:
    Condition: IsKMSKeyProvided
    DependsOn: updateKMSkeysFunction
    Type: Custom::updateCustomManagedKeys
    Properties:
      ServiceToken: !GetAtt 'updateKMSkeysFunction.Arn'
      key_id: !Ref KeyArn
      arn_credentials: !Ref IAMRoleARN
      use_cases: !Ref KeyUseCases
      reuse_key_for_cluster_volumes: !Ref KeyReuseForClusterVolumes

  # Databricks main Lambda for all E2 objects and workspace creation
  databricksApiFunction:
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Databricks account API.
      Handler: rest_client.handler
      Runtime: python3.8
      Role: !GetAtt 'functionRole.Arn'
      Timeout: 900
      Code:
        S3Bucket: !Ref 'LambdaZipsBucket'
        S3Key: !Sub '${QSS3KeyPrefix}functions/packages/lambda.zip' 
  
  # Databricks lambda for updating the Security Group Ids 
  updateIAMRoleFunction:
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Update IAM role policy document using the list of security group IDs.
      Handler: update_custommanagedvpc_iam_role.handler
      Runtime: python3.8
      Role: !GetAtt 'functionRole.Arn'
      Timeout: 60
      Code:
        S3Bucket: !Ref 'LambdaZipsBucket'
        S3Key: !Sub '${QSS3KeyPrefix}functions/packages/lambda.zip'

  # Databricks CMK lambda
  updateKMSkeysFunction:
    Condition: IsKMSKeyProvided
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: UUpdate CMK policy document for storage.
      Handler: update_custommanaged_cmk_policy.handler
      Runtime: python3.8
      Role: !GetAtt 'functionRole.Arn'
      Timeout: 60
      Code:
        S3Bucket: !Ref 'LambdaZipsBucket'
        S3Key: !Sub '${QSS3KeyPrefix}functions/packages/lambda.zip'

  # IAM Role for lambda function execution
  functionRole:
    Type: AWS::IAM::Role
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - EIAMPolicyWildcardResource
          ignore_reasons:
            EIAMPolicyWildcardResource: "Need to manage databricks workspaces"
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole 
      Policies:
        - PolicyName: kmsUpdateRole
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action: 
                  - 'kms:GetKeyPolicy'
                  - 'kms:PutKeyPolicy'
                Resource: '*' 
        - PolicyName: iamUpdateRole
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action: 
                  - 'iam:PutRolePolicy'
                Resource: '*'
      Tags:
        -
          Key: Name
          Value: !Sub '${TagValue}-IAMRole'

  # Resources to stage lambda.zip file
  LambdaZipsBucket:
    Type: AWS::S3::Bucket
  CopyZips:
    Type: Custom::CopyZips
    Properties:
      ServiceToken: !GetAtt 'CopyZipsFunction.Arn'
      DestBucket: !Ref 'LambdaZipsBucket'
      SourceBucket: !Ref 'QSS3BucketName'
      Prefix: !Ref 'QSS3KeyPrefix'
      Objects:
        - functions/packages/lambda.zip
  CopyZipsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: lambda-copier
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:DeleteObject
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${LambdaZipsBucket}/${QSS3KeyPrefix}*' 
      Tags:
        -
          Key: Name
          Value: !Sub '${TagValue}-IAMRole'


  CopyZipsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Copies objects from an S3 bucket to another destination.
      Handler: index.handler
      Runtime: python3.8
      Role: !GetAtt 'CopyZipsRole.Arn'
      Timeout: 240
      Code:
        ZipFile: |
          import json
          import logging
          import threading
          import boto3
          import cfnresponse
          def copy_objects(source_bucket, dest_bucket, prefix, objects):
              s3 = boto3.client('s3')
              for o in objects:
                  key = prefix + o
                  copy_source = {
                      'Bucket': source_bucket,
                      'Key': key
                  }
                  print('copy_source: %s' % copy_source)
                  print('dest_bucket = %s'%dest_bucket)
                  print('key = %s' %key)
                  s3.copy_object(CopySource=copy_source, Bucket=dest_bucket,
                        Key=key)
          def delete_objects(bucket, prefix, objects):
              s3 = boto3.client('s3')
              objects = {'Objects': [{'Key': prefix + o} for o in objects]}
              s3.delete_objects(Bucket=bucket, Delete=objects)
          def timeout(event, context):
              logging.error('Execution is about to time out, sending failure response to CloudFormation')
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None)
          def handler(event, context):
              # make sure we send a failure to CloudFormation if the function
              # is going to timeout
              timer = threading.Timer((context.get_remaining_time_in_millis()
                        / 1000.00) - 0.5, timeout, args=[event, context])
              timer.start()
              print('Received event: %s' % json.dumps(event))
              status = cfnresponse.SUCCESS
              try:
                  source_bucket = event['ResourceProperties']['SourceBucket']
                  dest_bucket = event['ResourceProperties']['DestBucket']
                  prefix = event['ResourceProperties']['Prefix']
                  objects = event['ResourceProperties']['Objects']
                  if event['RequestType'] == 'Delete':
                      delete_objects(dest_bucket, prefix, objects)
                  else:
                      copy_objects(source_bucket, dest_bucket, prefix, objects)
              except Exception as e:
                  logging.error('Exception: %s' % e, exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  timer.cancel()
                  cfnresponse.send(event, context, status, {}, None)
                  